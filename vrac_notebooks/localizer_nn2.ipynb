{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "remote = callbacks.RemoteMonitor(root='http://localhost:9000')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.optimizers import *\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from convnetskeras.customlayers import Softmax4D\n",
    "import gc\n",
    "from convnetskeras.convnets import preprocess_image_batch, convnet\n",
    "from convnetskeras.imagenet_tool import synset_to_dfs_ids\n",
    "import PIL\n",
    "from PIL import Image,ImageFilter, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "from matplotlib.patches import Ellipse\n",
    "from numpy.linalg import eig, inv\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from skimage import data, io, filters\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "import math\n",
    "from skimage.transform import (hough_line, hough_line_peaks,\n",
    "                               probabilistic_hough_line)\n",
    "import skimage\n",
    "import collections\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.layers.core import  Lambda, Merge\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from os import listdir\n",
    "from os.path import isfile, join, dirname\n",
    "from scipy.io import loadmat\n",
    "import gc\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers.core import  Lambda, Merge\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from os import listdir\n",
    "from os.path import isfile, join, dirname\n",
    "from scipy.io import loadmat\n",
    "import gc\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "# Credits to heuritech for their great code which was a great inspiration.\n",
    "# Some of the code comes directly from their repository.\n",
    "# You can look it up: https://github.com/heuritech/convnets-keras\n",
    "\n",
    "\n",
    "def depthfirstsearch(id_, out=None):\n",
    "    if out is None:\n",
    "        out = []\n",
    "    if isinstance(id_, int):\n",
    "        pass\n",
    "    else:\n",
    "        id_ = next(int(s[0]) for s in synsets if s[1][0] == id_)\n",
    "        \n",
    "    out.append(id_)\n",
    "    children = synsets[id_-1][5][0]\n",
    "    for c in children:\n",
    "        depthfirstsearch(int(c), out)\n",
    "    return out\n",
    "\t\n",
    "# This is to find all the outputs that correspond to the class we want.\n",
    "def synset_to_dfs_ids(synset):\n",
    "    ids = [x for x in depthfirstsearch(synset) if x <= 1000]\n",
    "    ids = [corr[x] for x in ids]\n",
    "    return ids\n",
    "\t\n",
    "# Keras doesn't have a 4D softmax. So we need this.\n",
    "class Softmax4D(Layer):\n",
    "    def __init__(self, axis=-1,**kwargs):\n",
    "        self.axis=axis\n",
    "        super(Softmax4D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        e = K.exp(x - K.max(x, axis=self.axis, keepdims=True))\n",
    "        s = K.sum(e, axis=self.axis, keepdims=True)\n",
    "        return e / s\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\t\t\n",
    "\n",
    "def get_dim(model, layer_index, input_shape=None):\n",
    "    \n",
    "    # Input shape is the shape of images used during training.\n",
    "    if input_shape is not None:\n",
    "        dummy_vector = np.zeros((1,) + input_shape)\n",
    "    else:\n",
    "        if model.layers[0].input_shape[2] is None:\n",
    "            raise ValueError('You must provide \\\"input_shape = (3,256,256)\\\" for example when calling the function.')\n",
    "        dummy_vector = np.zeros((1,) + model.layers[0].input_shape[1:])\n",
    "    \n",
    "    intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.layers[layer_index].output)\n",
    "    \n",
    "    out = intermediate_layer_model.predict(dummy_vector)\n",
    "    \n",
    "    return out.shape[1:]\n",
    "\t\n",
    "\n",
    "def from_config(layer, config_dic):\n",
    "    config_correct = {}\n",
    "    config_correct['class_name'] = type(layer)\n",
    "    config_correct['config'] = config_dic\n",
    "    return layer_from_config(config_correct)\n",
    "\t\n",
    "\n",
    "def add_to_model(x, layer):\n",
    "    new_layer = from_config(layer, layer.get_config())\n",
    "    x = new_layer(x)\n",
    "    if layer.get_weights() is not None:\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "    return x\n",
    "\t\n",
    "\n",
    "def layer_type(layer):\n",
    "    return str(layer)[10:].split(\" \")[0].split(\".\")[-1]\n",
    "\t\n",
    "\n",
    "def detect_configuration(model):\n",
    "    # must return the configuration and the number of the first pooling layer\n",
    "    \n",
    "    # Names (types) of layers from end to beggining\n",
    "    inverted_list_layers = [layer_type(layer) for layer in model.layers[::-1]]\n",
    "    \n",
    "    layer1 = None\n",
    "    layer2 = None \n",
    "    \n",
    "    i = len(model.layers)\n",
    "    \n",
    "    for layer in inverted_list_layers:\n",
    "        i -= 1\n",
    "        if layer2 is None:\n",
    "            if layer == \"GlobalAveragePooling2D\" or layer == \"GlobalMaxPooling2D\":\n",
    "                layer2 = layer\n",
    "\n",
    "            elif layer == \"Flatten\":\n",
    "                return \"local pooling - flatten\", i-1\n",
    "            \n",
    "        else:\n",
    "            layer1 = layer\n",
    "            break\n",
    "            \n",
    "    if layer1 == \"MaxPooling2D\" and layer2 == \"GlobalMaxPooling2D\":\n",
    "        return \"local pooling - global pooling (same type)\", i\n",
    "    elif layer1 == \"AveragePooling2D\" and layer2 == \"GlobalAveragePooling2D\":\n",
    "        return \"local pooling - global pooling (same type)\", i\n",
    "    \n",
    "    elif layer1 == \"MaxPooling2D\" and layer2 == \"GlobalAveragePooling2D\":\n",
    "        return \"local pooling - global pooling (different type)\", i+1\n",
    "    elif layer1 == \"AveragePooling2D\" and layer2 == \"GlobalMaxPooling2D\":\n",
    "        return \"local pooling - global pooling (different type)\", i+1\n",
    "    \n",
    "    else:\n",
    "        return \"global pooling\", i\n",
    "\t\t\n",
    "    \n",
    "def add_zeros(w, nb_zeros):\n",
    "    \n",
    "    n = w.shape[3]\n",
    "    indexes = np.array(range(1, n))\n",
    "    w1 = w\n",
    "    for i in range(nb_zeros):\n",
    "        w1 = np.insert(w1, indexes + i, 0, axis=2)\n",
    "    for i in range(nb_zeros):\n",
    "        w1 = np.insert(w1, indexes + i, 0, axis=3)\n",
    "    return w1\n",
    "\t\n",
    "    \n",
    "def insert_weights(layer, new_layer):\n",
    "    W,b = layer.get_weights()\n",
    "    n_filter,previous_filter,ax1,ax2 = new_layer.get_weights()[0].shape\n",
    "    ax1 = ax2 = int(np.sqrt(layer.get_weights()[0].shape[0]/new_layer.get_weights()[0].shape[1]))\n",
    "    new_W = W.reshape((previous_filter,ax1,ax2,n_filter))\n",
    "    new_W = new_W.transpose((3,0,1,2))\n",
    "    new_W = new_W[:,:,::-1,::-1]\n",
    "\t\n",
    "    \n",
    "    if ax1!=1:\n",
    "        insert_zeros = int((new_layer.get_weights()[0].shape[2] - ax1)/(ax1-1))\n",
    "        print(\"insert_zeros=\" + str(insert_zeros))\n",
    "        new_W =  add_zeros(new_W, insert_zeros)\n",
    "    \n",
    "    new_layer.set_weights([new_W,b])\n",
    "\t\n",
    "    \n",
    "def copy_last_layers(model, begin,x):\n",
    "    \n",
    "    i=begin\n",
    "    \n",
    "    for layer in model.layers[begin:]:\n",
    "        if layer_type(layer) == \"Dense\":\n",
    "            \n",
    "            if i == len(model.layers)-1:\n",
    "                x = add_reshaped_layer(layer,x,1, no_activation=True)\n",
    "            else:\n",
    "                x = add_reshaped_layer(layer,x,1)\n",
    "            \n",
    "        elif layer_type(layer) == \"Dropout\":\n",
    "            pass\n",
    "                \n",
    "        elif layer_type(layer) == \"Activation\" and i == len(model.layers)-1:\n",
    "            break\n",
    "               \n",
    "        else:\n",
    "            x = add_to_model(x, layer)\n",
    "        i+=1\n",
    "    \n",
    "    x = Softmax4D(axis=1,name=\"softmax\")(x)\n",
    "    return x\n",
    "    \n",
    "                \n",
    "def add_reshaped_layer(layer, x, size, no_activation=False, add_zeros = None):\n",
    "\n",
    "    conf = layer.get_config()\n",
    "    \n",
    "    if no_activation:\n",
    "        activation=\"linear\"\n",
    "    else:\n",
    "        activation=conf[\"activation\"]\n",
    "        \n",
    "    #size = int(np.sqrt(layer.get_weights()[0].shape[0]/conf[\"output_dim\"]))\n",
    "    \n",
    "    new_layer = Convolution2D(conf[\"output_dim\"],size,size, activation=activation, name=conf['name'])\n",
    "         \n",
    "        \n",
    "    x= new_layer(x)\n",
    "    # We transfer the weights:\n",
    "    insert_weights(layer, new_layer)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def to_heatmap(model, input_shape = None, delete = False):\n",
    "    \n",
    "    # there are four configurations possible:\n",
    "    # global pooling\n",
    "    # local pooling - flatten\n",
    "    # local pooling - global pooling (same type)\n",
    "    # local pooling - global pooling (different type)\n",
    "    \n",
    "    model_type, index = detect_configuration(model)\n",
    "    \n",
    "    print(\"Model type detected: \" + model_type)\n",
    "    \n",
    "    #new_layer.set_weights(model.layers[0].get_weights())\n",
    "    img_input = Input(shape=(3,None,None))\n",
    "   \n",
    "    # Inchanged part:\n",
    "    middle_model = Model(input=model.layers[1].input, output=model.layers[index-1].output)\n",
    "    \n",
    "    x = middle_model(img_input)\n",
    "    \n",
    "    print(\"Model cut at layer: \" + str(index))\n",
    "        \n",
    "    if model_type == \"global pooling\":\n",
    "        x = copy_last_layers(model, index+1,x)\n",
    "              \n",
    "    elif model_type == \"local pooling - flatten\":\n",
    "        \n",
    "        layer = model.layers[index]\n",
    "        dic = layer.get_config()\n",
    "        add_zeros = dic[\"strides\"][0] - 1\n",
    "        dic[\"strides\"] = (1,1)\n",
    "        new_pool = from_config(layer, dic)\n",
    "        x = new_pool(x)\n",
    "        \n",
    "        size = get_dim(model, index, input_shape)[1]\n",
    "        print(\"Pool size infered: \" + str(size))\n",
    "        \n",
    "        conv_size = size + (size-1) * add_zeros\n",
    "        \n",
    "        print(\"New convolution size: \" + str(conv_size))\n",
    "        \n",
    "        if index+2 != len(model.layers)-1:\n",
    "            x = add_reshaped_layer(model.layers[index+2],x,conv_size, add_zeros=add_zeros)\n",
    "        else:\n",
    "            x = add_reshaped_layer(model.layers[index+2],x,conv_size, add_zeros=add_zeros,no_activation=True)\n",
    "            \n",
    "        x = copy_last_layers(model, index+3,x)\n",
    "        \n",
    "        \n",
    "    elif model_type == \"local pooling - global pooling (same type)\":\n",
    "        \n",
    "        \n",
    "        dim = get_dim(model, index, input_shape=input_shape)\n",
    "\n",
    "        new_pool_size = model.layers[index].get_config()[\"pool_size\"][0] * dim[1]\n",
    "        \n",
    "        print(\"Pool size infered: \" + str(new_pool_size))\n",
    "        \n",
    "        x = AveragePooling2D(pool_size=(new_pool_size, new_pool_size), strides=(1,1)) (x)\n",
    "        x = copy_last_layers(model, index+2,x)\n",
    "        \n",
    "        \n",
    "    elif model_type == \"local pooling - global pooling (different type)\":\n",
    "        x= copy_last_layers(model, index+1,x)\n",
    "    else:\n",
    "        raise IndexError(\"no type for model: \" + str(model_type))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if delete:\n",
    "        del(model)\n",
    "        gc.collect()\n",
    "        print(\"Original model was deleted.\")\n",
    "    \n",
    "    return Model(img_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alb_labels.json', 'bet_labels.json', 'dol_labels.json', 'lag_labels.json', 'other_labels.json', 'shark_labels.json', 'yft_labels.json']\n",
      "dict_keys(['BET', 'DOL', 'ALB', 'SHARK', 'OTHER', 'LAG', 'YFT'])\n"
     ]
    }
   ],
   "source": [
    "jsons = [f for f in os.listdir('./') if '.json' in f and 'labels' in f]\n",
    "print(jsons)\n",
    "data_json = collections.defaultdict(int)\n",
    "for fjson in jsons:\n",
    "    with open(fjson,'r+') as f:\n",
    "        name = fjson.split('_')[0].upper()\n",
    "        data_json[name] = collections.defaultdict(list)\n",
    "        labels = json.load(f)\n",
    "        for label in labels:\n",
    "            img_name = label['filename'].split('/')[-1]\n",
    "            data_json[name][img_name] = label['annotations'] \n",
    "print(data_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
      "./localizer_nn2/RESTNET50/train/\n"
     ]
    }
   ],
   "source": [
    "root = './localizer_nn2/'\n",
    "model = 'RESTNET50/'\n",
    "train = 'train/'\n",
    "test = 'test/'\n",
    "obj = [direc for direc in os.listdir('./train/train/') if direc!='.DS_Store']\n",
    "print(obj)\n",
    "#create model dir\n",
    "try:\n",
    "    os.stat(root)\n",
    "except:\n",
    "    os.mkdir(root)\n",
    "try:\n",
    "    os.stat(root+model)\n",
    "except:\n",
    "    os.mkdir(root+model)    \n",
    "try:\n",
    "    os.stat(root+model+train)\n",
    "except:\n",
    "    os.mkdir(root+model+train)\n",
    "#create train and test\n",
    "try:\n",
    "    os.stat(root+model+train)\n",
    "except:\n",
    "    os.mkdir(root+model+train)\n",
    "try:\n",
    "    os.stat(root+model+test)\n",
    "except:\n",
    "    os.mkdir(root+model+test)\n",
    "#create classes   \n",
    "for direc in obj:\n",
    "    try:\n",
    "        os.stat(root+model+train+direc)\n",
    "    except:\n",
    "        os.mkdir(root+model+train+direc)\n",
    "for direc in obj:\n",
    "    try:\n",
    "        os.stat(root+model+test+direc)\n",
    "    except:\n",
    "        os.mkdir(root+model+test+direc)\n",
    "out_train = root+model+train\n",
    "out_test = root+model+test\n",
    "print(out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path_train = \"./train/train/\"\n",
    "imgs = [{'class':str(f)+'/','imgs_name':[os.listdir(root_path_train+f)]} for f in os.listdir(root_path_train) if f!='.DS_Store']\n",
    "imgs_N = np.array([len(os.listdir(root_path_train+f)) for f in os.listdir(root_path_train) if f!='.DS_Store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3777\n"
     ]
    }
   ],
   "source": [
    "L = imgs_N.sum()\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
