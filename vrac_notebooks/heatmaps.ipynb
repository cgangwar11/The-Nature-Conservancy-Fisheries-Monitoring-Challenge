{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from convnetskeras.convnets import preprocess_image_batch, convnet\n",
    "from convnetskeras.imagenet_tool import synset_to_dfs_ids\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "import itertools\n",
    "from convnetskeras.customlayers import Softmax4D\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def layer_type(layer):\n",
    "    return str(layer)[10:].split(\" \")[0].split(\".\")[-1]\n",
    "\n",
    "def detect_configuration(model):\n",
    "    # must return the configuration and the number of the first pooling layer\n",
    "    \n",
    "    # Names (types) of layers from end to beggining\n",
    "    inverted_list_layers = [layer_type(layer) for layer in model.layers[::-1]]\n",
    "    \n",
    "    layer1 = None\n",
    "    layer2 = None \n",
    "    \n",
    "    i = len(model.layers)\n",
    "    \n",
    "    for layer in inverted_list_layers:\n",
    "        i -= 1\n",
    "        if layer2 is None:\n",
    "            if layer == \"GlobalAveragePooling2D\" or layer == \"GlobalMaxPooling2D\":\n",
    "                layer2 = layer\n",
    "\n",
    "            elif layer == \"Flatten\":\n",
    "                return \"local pooling - flatten\", i\n",
    "            \n",
    "        else:\n",
    "            layer1 = layer\n",
    "            break\n",
    "            \n",
    "    if layer1 == \"MaxPooling2D\" and layer2 == \"GlobalMaxPooling2D\":\n",
    "        return \"local pooling - global pooling (same type)\", i\n",
    "    elif layer1 == \"AveragePooling2D\" and layer2 == \"GlobalAveragePooling2D\":\n",
    "        return \"local pooling - global pooling (same type)\", i\n",
    "    \n",
    "    elif layer1 == \"MaxPooling2D\" and layer2 == \"GlobalAveragePooling2D\":\n",
    "        return \"local pooling - global pooling (different type)\", i+1\n",
    "    elif layer1 == \"AveragePooling2D\" and layer2 == \"GlobalMaxPooling2D\":\n",
    "        return \"local pooling - global pooling (different type)\", i+1\n",
    "    \n",
    "    else:\n",
    "        return \"global pooling\", i\n",
    "    \n",
    "def insert_weights(layer, new_layer):\n",
    "    W,b = layer.get_weights()\n",
    "    print(W.shape)\n",
    "    n_filter,previous_filter,ax1,ax2 = new_layer.get_weights()[0].shape\n",
    "    new_W = W.reshape((previous_filter,ax1,ax2,n_filter))\n",
    "    new_W = new_W.transpose((3,0,1,2))\n",
    "    new_W = new_W[:,:,::-1,::-1]\n",
    "    new_layer.set_weights([new_W,b])\n",
    "    \n",
    "def copy_last_layers(model, new_model, begin):\n",
    "    \n",
    "    i=begin\n",
    "    \n",
    "    for layer in model.layers[begin:]:\n",
    "        if layer_type(layer) == \"Dense\":\n",
    "                \n",
    "            add_reshaped_layer(layer, new_model, 1)\n",
    "                \n",
    "        elif layer_type(layer) == \"Activation\" and i == len(model.layers)-1:\n",
    "            break\n",
    "               \n",
    "        else:\n",
    "            print(i)\n",
    "            print(len(model.layers))\n",
    "            print(layer_type(layer))\n",
    "            new_model.add(layer)\n",
    "        i+=1\n",
    "    \n",
    "    model.add(Softmax4D(axis=1,name=\"softmax\"))\n",
    "    \n",
    "                \n",
    "def add_reshaped_layer(layer, new_model, size):\n",
    "\n",
    "    conf = layer.get_config()\n",
    "    \n",
    "    new_layer = Convolution2D(conf[\"output_dim\"],size,size,activation=conf[\"activation\"], name=conf['name'])\n",
    "         \n",
    "        \n",
    "    new_model.add(new_layer)\n",
    "    # We transfer the weights:\n",
    "    insert_weights(layer, new_layer)\n",
    "    \n",
    "\n",
    "def to_heatmap_sequential(model, size=None, increased_resolution=False, sequential=False, delete = False):\n",
    "    \n",
    "    # First, we need to find which layers need special attention. \n",
    "    # We do that by examining the last layers and stop once we see a pool layer. (what if 2 pooling layers?)\n",
    "    # eg: averagePool -> globalPool?\n",
    "    # This is for later. Right now, we need a simple heatmap.\n",
    "    \n",
    "    # there are four configurations possible:\n",
    "    # global pooling\n",
    "    # local pooling - flatten\n",
    "    # local pooling - global pooling (same type)\n",
    "    # local pooling - global pooling (different type)\n",
    "    \n",
    "    model_type, index = detect_configuration(model)\n",
    "    \n",
    "    # We reproduce the beggining of the network.\n",
    "    new_model = Sequential()\n",
    "    for layer in model.layers[:index]:\n",
    "        new_model.add(layer)\n",
    "        \n",
    "    if model_type == \"global pooling\":\n",
    "        copy_last_layers(model, new_model, index+1)\n",
    "              \n",
    "    elif model_type == \"local pooling - flatten\":\n",
    "        \n",
    "        add_reshaped_layer(model.layers[index+1], new_model, size)\n",
    "        \n",
    "        copy_last_layers(model, new_model, index+2)\n",
    "        \n",
    "        \n",
    "    elif model_type == \"local pooling - global pooling (same type)\":\n",
    "        copy_last_layers(model, new_model, index+2)\n",
    "    elif model_type == \"local pooling - global pooling (different type)\":\n",
    "        copy_last_layers(model, new_model, index+1)\n",
    "    else:\n",
    "        raise IndexError(\"no type for model: \" + str(model_type))\n",
    "    \n",
    "    if delete:\n",
    "        del(model)\n",
    "        gc.collect()\n",
    "    \n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25088/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0e5c957ed7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_heatmap_sequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "new_model = to_heatmap_sequential(model, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.layers[-4].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(model.layers[-1])[10:].split(\" \")[0].split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.layers[-31].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_heatmap_functional(model, resolution_x2=False, sequential=False):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pop_layer(model):\n",
    "    if not model.outputs:\n",
    "        raise Exception('Sequential model cannot be popped: model is empty.')\n",
    "\n",
    "    model.layers.pop()\n",
    "    if not model.layers:\n",
    "        model.outputs = []\n",
    "        model.inbound_nodes = []\n",
    "        model.outbound_nodes = []\n",
    "    else:\n",
    "        model.layers[-1].outbound_nodes = []\n",
    "        model.outputs = [model.layers[-1].output]\n",
    "    model.built = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "l = []\n",
    "\n",
    "root = './train/'\n",
    "pattern = \"*.jpg\"\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if fnmatch(name, pattern):\n",
    "            l.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 7\n",
    "dest = [ string[:s] + \"_heatmap\" + string[s:] for string in l]\n",
    "dest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dest_pickle = [ string[:s] + \"_pickle\" + string[s:] for string in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_heatmap(path,model):\n",
    "        \n",
    "    im = preprocess_image_batch([path], color_mode=\"bgr\")\n",
    "\n",
    "    out = model.predict(im)\n",
    "\n",
    "    s = \"n02512053\"\n",
    "    ids = synset_to_dfs_ids(s)\n",
    "    heatmap = out[0,ids].sum(axis=0)\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def save_heatmaps(model,l,dest_pickle, dest):\n",
    "    for i in tq(range(len(l))):\n",
    "\n",
    "        heatmap = get_heatmap(l[i], model)\n",
    "\n",
    "        im = preprocess_image_batch([l[i]], color_mode=\"bgr\")\n",
    "\n",
    "        plt.imsave(dest[i],heatmap)\n",
    "        \n",
    "        with open(dest_pickle[i],\"wb\") as f:\n",
    "            pickle.dump(heatmap, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "('Error allocating 1644167168 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).', \"you might consider using 'theano.shared(..., borrow=True)'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3ab63d67ecb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"custom_dense_1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dense_2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dense_3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    322\u001b[0m                  output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    323\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Exception('All layers in a Sequential model '\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    489\u001b[0m                                      '`layer.build(batch_input_shape)`')\n\u001b[1;32m    490\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid dim_ordering: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'{}_W'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_filter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'{}_b'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\initializations.py\u001b[0m in \u001b[0;36mglorot_uniform\u001b[0;34m(shape, name, dim_ordering)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mfan_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfan_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_ordering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfan_in\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\initializations.py\u001b[0m in \u001b[0;36muniform\u001b[0;34m(shape, scale, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36mrandom_uniform_variable\u001b[0;34m(shape, low, high, dtype, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrandom_uniform_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_FLOATX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     return variable(np.random.uniform(low=low, high=high, size=shape),\n\u001b[0;32m--> 142\u001b[0;31m                     dtype=dtype, name=name)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\keras\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\theano\\theano\\compile\\sharedvalue.py\u001b[0m in \u001b[0;36mshared\u001b[0;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[0;32m--> 247\u001b[0;31m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Software\\Anaconda3\\lib\\site-packages\\theano\\theano\\sandbox\\cuda\\var.py\u001b[0m in \u001b[0;36mfloat32_shared_constructor\u001b[0;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[1;31m# type.broadcastable is guaranteed to be a tuple, which this next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[1;31m# function requires\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mdeviceval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_support_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: ('Error allocating 1644167168 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).', \"you might consider using 'theano.shared(..., borrow=True)'\")"
     ]
    }
   ],
   "source": [
    "pb = tq(total=47)\n",
    "old_model = convnet('vgg_19',weights_path=\"vgg19_weights.h5\", heatmap=True)\n",
    "pb.update()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(3,None,None)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_4'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_4'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_4'))\n",
    "model.add(MaxPooling2D((2,2), strides=(1,1)))\n",
    "\n",
    "model.add(Convolution2D(4096,14,14,activation=\"relu\",name=\"custom_dense_1\"))\n",
    "model.add(Convolution2D(4096,1,1,activation=\"relu\",name=\"dense_2\"))\n",
    "model.add(Convolution2D(1000,1,1,name=\"dense_3\"))\n",
    "model.add(Softmax4D(axis=1,name=\"softmax\"))\n",
    "\n",
    "pb.update()\n",
    "\n",
    "for layer in model.layers:\n",
    "    \n",
    "    if layer.name.startswith(\"custom_\"):\n",
    "        orig_layer = old_model.get_layer(layer.name[7:])\n",
    "        \n",
    "        with open(\"big_matrix.p\", \"rb\") as f:\n",
    "            new_weights = pickle.load(f)\n",
    "        \n",
    "        layer.set_weights([new_weights, orig_layer.get_weights()[1]])\n",
    "    elif layer.name.startswith(\"conv\") or layer.name.startswith(\"dense\"):\n",
    "        orig_layer = old_model.get_layer(layer.name)\n",
    "        layer.set_weights(orig_layer.get_weights())\n",
    "    pb.update()\n",
    "\n",
    "#model.compile(optimizer='sgd', loss='mse')\n",
    "pb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model = convnet('vgg_19',weights_path=\"vgg19_weights.h5\", heatmap=False)\n",
    "model.compile(optimizer=sgd, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(type(model.layers[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"big_matrix.p\", \"rb\") as f:\n",
    "    new_weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.layers[-4].get_weights()[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = preprocess_image_batch([\"./train/ALB/img_00110.jpg\"], color_mode=\"bgr\")\n",
    "\n",
    "out = model.predict(im)\n",
    "\n",
    "s = \"n02512053\"\n",
    "ids = synset_to_dfs_ids(s)\n",
    "heatmap = out[0,ids].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(heatmap, interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = preprocess_image_batch([\"./train/ALB/img_00136.jpg\"], color_mode=\"bgr\")\n",
    "\n",
    "out = model.predict(im)\n",
    "\n",
    "s = \"n02512053\"\n",
    "ids = synset_to_dfs_ids(s)\n",
    "heatmap = out[0,ids].sum(axis=0)\n",
    "plt.imshow(heatmap, interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_weights = model.layers[-4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = np.zeros(my_weights[0].shape[:-2] + (14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 14, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 3 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9847e5246fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 3 with size 3"
     ]
    }
   ],
   "source": [
    "c = tq(total=4096*512*14*14)\n",
    "for i, j, k, l in itertools.product(range(new.shape[0]),range(new.shape[1]),range(new.shape[2]),range(new.shape[3])):\n",
    "    if k%2 == 0 or l%2 == 0:\n",
    "        new[i,j,k,l] = 0\n",
    "    else:\n",
    "        new[i,j,k,l] = my_weights[0][i,j, int((k-1)/2), int((l-1)/2)]\n",
    "    c.update()\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"big_matrix_2.p\", \"wb\") as f:\n",
    "    pickle.dump(new, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_heatmaps(model,l,dest_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_heatmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bb8f55efafc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_heatmap' is not defined"
     ]
    }
   ],
   "source": [
    "a = get_heatmap(l[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "\n",
    "def binarize(value, threshold):\n",
    "    if value < threshold:\n",
    "        return 0\n",
    "    if value >= threshold:\n",
    "        return 1\n",
    "np_binarize = np.vectorize(binarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = np_binarize(a, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(b, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_class(value, old, new):\n",
    "    if value == old:\n",
    "        return new\n",
    "    else:\n",
    "        return value\n",
    "np_change_class = np.vectorize(change_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stuff(mat,i,j,islands_list, count, islands_matrix):\n",
    "    if mat[i][j] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        before = int(islands_matrix[i][j-1])\n",
    "        above = int(islands_matrix[i-1][j])\n",
    "        if before == 0:\n",
    "            if  above== 0:\n",
    "                islands_matrix[i][j] = count\n",
    "                count +=1\n",
    "                islands_list.append([(j,i)])\n",
    "            else: \n",
    "                islands_matrix[i][j] = above\n",
    "                islands_list[above-1].append((j,i))\n",
    "        \n",
    "        else:\n",
    "            if above == 0:\n",
    "                islands_matrix[i][j] = before\n",
    "                islands_list[before-1].append((j,i))\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                if above == before:\n",
    "                    islands_matrix[i][j] = before\n",
    "                    islands_list[before-1].append((j,i))\n",
    "                \n",
    "                #it's on.\n",
    "                else:\n",
    "                    old = max((above,before))\n",
    "                    new = min((above,before))\n",
    "                    \n",
    "                    islands_matrix[i][j] = new\n",
    "                    islands_matrix = np_change_class(islands_matrix, old, new)\n",
    "                    islands_list[new-1] += islands_list[old-1]\n",
    "                    islands_list[new-1].append((j,i))\n",
    "                    islands_list[old-1] = []\n",
    "                    \n",
    "    return islands_list, count, islands_matrix        \n",
    "\n",
    "\n",
    "# The not fun part\n",
    "def group_by_island(matrix):\n",
    "    \n",
    "    h, l = np.shape(matrix)\n",
    "    border = np.array([[0]*(l)])\n",
    "    mat = np.concatenate((border,matrix), axis=0)\n",
    "    border = np.array([[0]*(h+1)])\n",
    "    mat = np.concatenate((border.T, mat), axis=1)\n",
    "    \n",
    "    islands_matrix =np.zeros((h+1,l+1))\n",
    "    count = 1\n",
    "    islands_list = []\n",
    "    \n",
    "    for i in range(1,h+1):\n",
    "        for j in range(1, l+1):\n",
    "            islands_list, count, islands_matrix = stuff(mat,i,j,islands_list, count, islands_matrix)\n",
    "\n",
    "    \n",
    "    \n",
    "    return islands_matrix, islands_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "islands_matrix, islands_list = group_by_island(b)\n",
    "plt.imshow(islands_matrix, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_extremes(list_coordinates):\n",
    "    max_x = max(list_coordinates, key = lambda x: x[0])[0]\n",
    "    min_x = min(list_coordinates, key = lambda x: x[0])[0]\n",
    "    max_y = max(list_coordinates, key = lambda x: x[1])[1]\n",
    "    min_y = min(list_coordinates, key = lambda x: x[1])[1]\n",
    "    \n",
    "    #print(min_x, min_y, max_x, max_y )\n",
    "    \n",
    "    return min_x - 1, min_y - 1, max_x-min_x + 1, max_y-min_y + 1\n",
    "\n",
    "\n",
    "def get_rectangles(islands_list, area_limit=4):\n",
    "    list_coordinates = []\n",
    "    for island in islands_list:\n",
    "        if len(island) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            coordinates = find_extremes(island)\n",
    "            #print(coordinates)\n",
    "            #print(coordinates[2]*coordinates[3])\n",
    "            if coordinates[2]*coordinates[3] >=area_limit:\n",
    "                #print(coordinates)\n",
    "                list_coordinates.append(coordinates)\n",
    "            \n",
    "    return list_coordinates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rectangles = get_rectangles(islands_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rectangles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_coordinates(rectangles, heatmap, path_image):\n",
    "    y, x = np.shape(heatmap)\n",
    "    #print('y=' + str(y))\n",
    "    #print(\"x=\" + str(x))\n",
    "    \n",
    "    img = np.array(Image.open(path_image))\n",
    "    y1, x1, channel = np.shape(img)\n",
    "    #print(\"x1=\" + str(x1))\n",
    "    #print(\"y1=\" + str(y1))\n",
    "    \n",
    "    ratio_x = x1/x\n",
    "    ratio_y = y1/y\n",
    "    \n",
    "    converted_rectangles = []\n",
    "    for r in rectangles:\n",
    "        converted_rectangles.append((r[0]*ratio_x, r[1]*ratio_y, r[2]*ratio_x, r[3]*ratio_y,))\n",
    "        \n",
    "    return converted_rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rectangles = convert_coordinates(rectangles, a, l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dic = {\"filename\": l[0], \"rectangles\": new_rectangles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0.4\n",
    "def from_pickle_to_rectangle(img_path, pickle_path):\n",
    "    \n",
    "    with open(pickle_path,\"rb\") as f:\n",
    "        heatmap = pickle.load(f)\n",
    "        \n",
    "    # Here we will modify the heatmap, removing activation on the extremes of the image, because\n",
    "    # cause the fish detector to activate too often for some reason.\n",
    "    line_zeros = np.zeros(np.shape(heatmap)[0])\n",
    "    column_zeros = np.zeros(np.shape(heatmap)[1])\n",
    "    heatmap[:,0] = line_zeros\n",
    "    heatmap[:,-1] = line_zeros\n",
    "    heatmap[0,] = column_zeros\n",
    "    heatmap[-1,:] = column_zeros\n",
    "    \n",
    "        \n",
    "    #print(np.shape(heatmap))\n",
    "    threshold = c*np.max(heatmap)\n",
    "    binary_map = np_binarize(heatmap, threshold)\n",
    "    islands_matrix, islands_list = group_by_island(binary_map)\n",
    "    rectangles = get_rectangles(islands_list, area_limit=4)\n",
    "    #print(rectangles[0])\n",
    "    new_rectangles = convert_coordinates(rectangles, heatmap, img_path)\n",
    "    #print(new_rectangles[0])\n",
    "    new_dic = {\"path\": img_path, \"rectangles\": new_rectangles, \"p_fish\": np.max(heatmap)}\n",
    "    \n",
    "    \n",
    "    return new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_rectangles_images = []\n",
    "for i in tq(range(len(l))):\n",
    "    dic = from_pickle_to_rectangle(l[i], dest_pickle[i])\n",
    "    list_rectangles_images.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "l = []\n",
    "\n",
    "root = './test_stg1/'\n",
    "pattern = \"*.jpg\"\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if fnmatch(name, pattern):\n",
    "            l.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 11\n",
    "dest = [ string[:s] + \"_heatmap\" + string[s:] for string in l]\n",
    "dest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dest_pickle = [string[:s] + \"_pickle\" + string[s:] for string in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save_heatmaps(model,l,dest_pickle, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_rectangles_images = []\n",
    "for i in tq(range(len(l))):\n",
    "    dic = from_pickle_to_rectangle(l[i], dest_pickle[i])\n",
    "    list_rectangles_images.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"test_rectangles.p\", \"wb\") as f:\n",
    "    pickle.dump(list_rectangles_images,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(50,70):\n",
    "    dic = list_rectangles_images[i]\n",
    "    img = Image.open(dic[\"path\"])\n",
    "    print(str(dic[\"p_fish\"]*100) + \"        \" + dic[\"path\"])\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    with open(dest_pickle[i], \"rb\") as f:\n",
    "        heatmap = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    line_zeros = np.zeros(np.shape(heatmap)[0])\n",
    "    column_zeros = np.zeros(np.shape(heatmap)[1])\n",
    "    heatmap[:,0] = line_zeros\n",
    "    heatmap[:,-1] = line_zeros\n",
    "    heatmap[0,] = column_zeros\n",
    "    heatmap[-1,:] = column_zeros\n",
    "        \n",
    "    plt.imshow(heatmap, interpolation='none')\n",
    "    plt.show()\n",
    "    \n",
    "    threshold = c*np.max(heatmap)\n",
    "    binary_map = np_binarize(heatmap, threshold)\n",
    "    \n",
    "    plt.imshow(binary_map, interpolation='none')\n",
    "    plt.show()\n",
    "    \n",
    "    for rec in dic[\"rectangles\"]:\n",
    "        print(\"fish found: \" + str(rec))\n",
    "        img2 = img.crop((rec[0],rec[1], rec[0] + rec[2], rec[1] + rec[3]))\n",
    "        plt.imshow(img2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sea_rectangle(predict):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We make the assumpption that the standart image size is 57x101\n",
    "    # the order is x top left, y top left, x bottom right, y bottom right\n",
    "    # This is the string order, not the numerical order! beware!\n",
    "    if predict ==1:\n",
    "        rectangle = [[58,0,100,56]]\n",
    "    if predict ==10:\n",
    "        rectangle = [[0,45,15,56]]\n",
    "    if predict ==11:\n",
    "        rectangle = [[80,0,100,56],[0,0,20,20]]\n",
    "    if predict ==12:\n",
    "        rectangle = [[0,0,20,56]]\n",
    "    if predict ==13:\n",
    "        rectangle = [[0,30,100,56]]\n",
    "    if predict ==14:\n",
    "        rectangle = [[85,0,100,56],[0,40,30,56]]\n",
    "    if predict ==15:\n",
    "        rectangle = [[0,0,100,15],[0,0,30,30]]\n",
    "    if predict ==16:\n",
    "        rectangle = [[0,0,10,56]]\n",
    "    if predict ==17:\n",
    "        rectangle = [[55,0,100,56]]\n",
    "    if predict ==2:\n",
    "        rectangle = [[50,0,100,56]]\n",
    "    if predict ==3:\n",
    "        rectangle = [[0,0,25,56]]\n",
    "    if predict ==4:\n",
    "        rectangle = [[0,0,100,15]]\n",
    "    if predict ==5:\n",
    "        rectangle = [[0,0,100,20],[65,0,100,56]]\n",
    "    if predict ==6:\n",
    "        rectangle = [[0,0,100,25],[0,0,20,56]]\n",
    "    if predict ==7:\n",
    "        rectangle = [[0,0,20,56],[75,0,100,20]]\n",
    "    if predict ==8:\n",
    "        rectangle = [[0,0,20,56]]\n",
    "    if predict ==9:\n",
    "        rectangle = [[80,0,100,56]]\n",
    "    return rectangle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"cool_model_boats_id.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_patches(heatmap, rectangles):\n",
    "    y_map, x_map = np.shape(heatmap)\n",
    "    for rectangle in rectangles: \n",
    "        x_min = int(rectangle[0] *x_map/100)\n",
    "        x_max = int(rectangle[2] *x_map/100)\n",
    "        y_min = int(rectangle[1] *y_map/56)\n",
    "        y_max = int(rectangle[3] *y_map/56)\n",
    "        \n",
    "        for i in range(y_map):\n",
    "            for j in range(x_map):\n",
    "                if y_min<=i<y_max and x_min<=j<x_max:\n",
    "                    heatmap[i,j] = 0\n",
    "                    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hide_sea(heatmap, img_path, treashold = 0.87):\n",
    "    img = load_img(img_path,grayscale=True, target_size=(64,64))\n",
    "    img = img_to_array(img)\n",
    "    \n",
    "    probabilities = model.predict(np.array([img,]))[0][1:]\n",
    "    #print(probabilities)\n",
    "    \n",
    "    if np.max(probabilities) >= treashold:\n",
    "        rectangles = get_sea_rectangle(np.argmax(probabilities) + 1)\n",
    "        heatmap = remove_patches(heatmap, rectangles)\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0.4\n",
    "def from_pickle_to_rectangle(img_path, pickle_path, plot=False):\n",
    "    \n",
    "    with open(pickle_path,\"rb\") as f:\n",
    "        heatmap = pickle.load(f)\n",
    "        \n",
    "    # Here we will modify the heatmap, removing activation on the extremes of the image, because\n",
    "    # cause the fish detector to activate too often for some reason.\n",
    "    line_zeros = np.zeros(np.shape(heatmap)[0])\n",
    "    column_zeros = np.zeros(np.shape(heatmap)[1])\n",
    "    heatmap[:,0] = line_zeros\n",
    "    heatmap[:,-1] = line_zeros\n",
    "    heatmap[0,] = column_zeros\n",
    "    heatmap[-1,:] = column_zeros\n",
    "    \n",
    "    if plot:\n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        plt.imshow(heatmap, interpolation='none')\n",
    "        plt.show()\n",
    "    \n",
    "    heatmap = hide_sea(heatmap, img_path)\n",
    "    \n",
    "    if plot:\n",
    "        plt.imshow(heatmap, interpolation='none')\n",
    "        plt.show()\n",
    "        \n",
    "    #print(np.shape(heatmap))\n",
    "    threshold = c*np.max(heatmap)\n",
    "    binary_map = np_binarize(heatmap, threshold)\n",
    "    \n",
    "    if plot:\n",
    "        plt.imshow(binary_map, interpolation='none')\n",
    "        plt.show()\n",
    "    \n",
    "    islands_matrix, islands_list = group_by_island(binary_map)\n",
    "    rectangles = get_rectangles(islands_list, area_limit=4)\n",
    "    new_rectangles = convert_coordinates(rectangles, heatmap, img_path)\n",
    "    new_dic = {\"path\": img_path, \"rectangles\": new_rectangles, \"p_fish\": np.max(heatmap)}\n",
    "    \n",
    "    if plot:\n",
    "        for rec in new_dic[\"rectangles\"]:\n",
    "            print(\"fish found: \" + str(rec))\n",
    "            img2 = img.crop((rec[0],rec[1], rec[0] + rec[2], rec[1] + rec[3]))\n",
    "            plt.imshow(img2)\n",
    "            plt.show()\n",
    "    \n",
    "    return new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_rectangles_images = []\n",
    "for i in tq(range(50,70)):\n",
    "    dic = from_pickle_to_rectangle(l[i], dest_pickle[i], True)\n",
    "    list_rectangles_images.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "0d3b830ea76b49bf948ed29eae49e23d": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "c2515b8956af4d3a839f189d35691981": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
