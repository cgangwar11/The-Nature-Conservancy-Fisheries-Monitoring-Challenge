{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'meta' from 'C:\\\\Users\\\\yolo\\\\Desktop\\\\fish\\\\meta.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import h_gen\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import utils\n",
    "import importlib\n",
    "import meta\n",
    "metadata = utils.load(\"./variables/metadata.p\")\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "remote = callbacks.RemoteMonitor(root='http://localhost:9000')\n",
    "from keras.layers.normalization import *\n",
    "from keras.models import load_model\n",
    "from shutil import copyfile\n",
    "importlib.reload(utils)\n",
    "importlib.reload(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(nb_images, train_dir):\n",
    "    temp = train_dir.find(\"train\")\n",
    "    test_dir = train_dir[:temp] + \"test\" + train_dir[temp+5:]\n",
    "    \n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "        \n",
    "    classes = os.listdir(train_dir)\n",
    "        \n",
    "    train_dir = [train_dir + x + \"/\" for x in classes]\n",
    "    test_dir = [test_dir + x + \"/\" for x in classes]\n",
    "    \n",
    "    for folder in test_dir:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    \n",
    "    for train_class, test_class in zip(train_dir, test_dir):\n",
    "        files = os.listdir(train_class)[:nb_images]\n",
    "        for file in files:\n",
    "            copyfile(train_class + file,test_class + file)\n",
    "            os.remove(train_class + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_train_test(30,\"cropped_train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.01,\n",
    "    height_shift_range=0.01,\n",
    "    shear_range=0.0005,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15997 images belonging to 8 classes.\n",
      "Found 240 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "target_size=(224,224)\n",
    "train_generator = train_datagen.flow_from_directory('cropped_gen', batch_size=32, \n",
    "                                                    target_size=target_size)\n",
    "validation_generator = test_datagen.flow_from_directory('cropped_test', batch_size=32, target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base = VGG16(include_top=False)\n",
    "img_input = Input(shape=(3,None,None))\n",
    "x = base(img_input)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x =Convolution2D(64, 3,3, activation=\"relu\", border_mode=\"same\")(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x =Convolution2D(64, 3,3, activation=\"relu\", border_mode=\"same\")(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x =Convolution2D(32, 3,3, activation=\"relu\", border_mode=\"same\")(x)\n",
    "x = BatchNormalization(axis=1)(x)\n",
    "x =Convolution2D(8, 3,3, border_mode=\"same\")(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation(\"softmax\")(x)\n",
    "model = Model(img_input, x)\n",
    "for layer in base.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "base.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 3, None, None) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "vgg16 (Model)                    (None, 512, None, Non 14714688    input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 512, None, Non 2048        vgg16[1][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, None, None 294976      batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, None, None 256         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, None, None 36928       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 64, None, None 256         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 32, None, None 18464       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 32, None, None 128         convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 8, None, None) 2312        batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "globalaveragepooling2d_1 (Global (None, 8)             0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 8)             0           globalaveragepooling2d_1[0][0]   \n",
      "====================================================================================================\n",
      "Total params: 15,070,056\n",
      "Trainable params: 354,024\n",
      "Non-trainable params: 14,716,032\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#opt = SGD(lr=0.01, decay=0.01)\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpe02t5j09/m9a6bd0eb5ed5c92e91261282fc495cb4.lib and object C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpe02t5j09/m9a6bd0eb5ed5c92e91261282fc495cb4.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpnt2rpg86/m848dd898e26d545ff6290e3aa98de3d5.lib and object C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpnt2rpg86/m848dd898e26d545ff6290e3aa98de3d5.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpn98_ynv7/mcaee517fdbbfe5601d70389b5e9a720a.lib and object C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmpn98_ynv7/mcaee517fdbbfe5601d70389b5e9a720a.exp\n",
      "\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1527: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a51358357ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         nb_val_samples=240, callbacks=[remote])\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1507\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1509\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[1;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator,\n",
    "        samples_per_epoch=512,\n",
    "        nb_epoch=100,\n",
    "        validation_data=validation_generator,\n",
    "        verbose=0,\n",
    "        nb_val_samples=240, callbacks=[remote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in base.layers:\n",
    "    layer.trainable=True\n",
    "\n",
    "base.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(\"yoloswagdu113.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = load_model(\"yoloswagdu113.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_pictures(train_folder, class_folder, generator,total_images, \n",
    "                        batch_size = 32, target_size = (224,224), color_mode = \"rgb\"):\n",
    "    #count = len(os.listdir(train_folder + \"/\" + class_folder))\n",
    "    count = 0  \n",
    "    gen_folder = train_folder + \"_gen/\"\n",
    "        \n",
    "    if not os.path.exists(gen_folder):\n",
    "        os.makedirs(gen_folder)\n",
    "        \n",
    "    if not os.path.exists(gen_folder + class_folder):\n",
    "        os.makedirs(gen_folder + class_folder)\n",
    "        \n",
    "    \n",
    "    progbar = tq(total = total_images, leave=False)\n",
    "    progbar.update(count)\n",
    "    \n",
    "    \n",
    "    for X in generator.flow_from_directory(train_folder, target_size=target_size,\n",
    "                                             classes=[class_folder], \n",
    "                                             save_to_dir=train_folder + \"_gen/\" + class_folder, \n",
    "                                             batch_size=batch_size, color_mode=color_mode):\n",
    "        count +=batch_size\n",
    "        progbar.update(batch_size)\n",
    "        if count >= total_images:\n",
    "            break\n",
    "    progbar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equilibrate_dataset(folder, nb_images_per_class, generator):\n",
    "    c = 0\n",
    "    while c < nb_images_per_class:\n",
    "        for string in os.listdir(folder):\n",
    "            create_new_pictures(folder, string, generator, 2000, 1, (224,224))\n",
    "        c += 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 1 classes.\n",
      "Found 277 images belonging to 1 classes.\n",
      "Found 94 images belonging to 1 classes.\n",
      "Found 71 images belonging to 1 classes.\n",
      "Found 115184 images belonging to 1 classes.\n",
      "Found 359 images belonging to 1 classes.\n",
      "Found 166 images belonging to 1 classes.\n",
      "Found 793 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "equilibrate_dataset(\"cropped\", 2000, test_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
