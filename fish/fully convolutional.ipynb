{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "DEBUG: nvcc STDOUT nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "mod.cu\n",
      "   Creating library C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmp857z_yum/m91973e5c136ea49268a916ff971b7377.lib and object C:/Users/yolo/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-SP0-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-3.5.2-64/tmp857z_yum/m91973e5c136ea49268a916ff971b7377.exp\n",
      "\n",
      "Using gpu device 0: GeForce GTX 970 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\theano\\sandbox\\cuda\\__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'meta' from 'C:\\\\Users\\\\yolo\\\\Desktop\\\\fish\\\\meta.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import h_gen\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import utils\n",
    "import importlib\n",
    "import meta\n",
    "metadata = utils.load(\"./variables/metadata.p\")\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import os\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "remote = callbacks.RemoteMonitor(root='http://localhost:9000')\n",
    "from shutil import copyfile\n",
    "importlib.reload(utils)\n",
    "importlib.reload(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this generates the rectangles to crop.\n",
    "# size is the average size of rectangles to crop\n",
    "# there is also a variation in the size that can be tuned\n",
    "def rectangle_generator(height, width, size=150, variation=0.3, rectangles=[]):\n",
    "    xs = np.linspace(0, width, 8)\n",
    "    ys = np.linspace(0, height, 8)\n",
    "    for x in xs[1:-1]:\n",
    "        for y in ys[1:-1]:\n",
    "            sides = np.random.uniform(size*(1-variation), size*(1+variation), (2,))\n",
    "            #print(\"x=\" + str(x) + \"   y=\" + str(y))\n",
    "            rect = (int(x-sides[0]/2), int(y-sides[1]/2), int(x+sides[0]/2), int(y+sides[1]/2))\n",
    "            overlapped = False\n",
    "            for rectangle in rectangles:\n",
    "                overlapped = meta.overlap(rectangle, rect)\n",
    "                if overlapped:\n",
    "                    break\n",
    "            if not overlapped:\n",
    "                yield rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We create the folders\n",
    "h_gen.mk(\"cropped\")\n",
    "for key, value in meta.classes_dictionary.items():\n",
    "    h_gen.mk(\"cropped/\" + key)\n",
    "\n",
    "# We crop the images\n",
    "for key, value in tq(metadata.items()):\n",
    "    \n",
    "    img = Image.open(value[\"path\"])\n",
    "    rectangles = []\n",
    "    # First, we crop the fishes\n",
    "    if \"rectangles\" in value:\n",
    "        rectangles = value[\"rectangles\"]\n",
    "        for i, rect in enumerate(rectangles):\n",
    "            path_to_save = \"cropped/\" + value[\"class\"] + \"/\" + value[\"filename\"][:-4] + \"_\" + str(i) + value[\"filename\"][-4:]\n",
    "            \n",
    "            # We want the fishes to be in squares, so we need to find the center\n",
    "            side = max(rect[\"width\"], rect[\"height\"])\n",
    "            x = rect[\"x\"] + rect[\"width\"]/2\n",
    "            y = rect[\"y\"] + rect[\"height\"]/2\n",
    "            m = 15\n",
    "            \n",
    "            cropped = img.crop((x-side/2-m , y-side/2-m, x+side/2+m , y+side/2+m))\n",
    "            cropped.save(path_to_save)\n",
    "    \n",
    "    # Now we do crop of the background\n",
    "    for i, rect in enumerate(rectangle_generator(value[\"width\"], value[\"height\"], 230, 0.3, rectangles)):\n",
    "        cropped = img.crop(rect)\n",
    "        path_to_save = \"cropped/NoF/\" + value[\"filename\"][:-4] + \"_\" + str(i) + value[\"filename\"][-4:]\n",
    "        cropped.save(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(nb_images, train_dir):\n",
    "    temp = train_dir.find(\"train\")\n",
    "    test_dir = train_dir[:temp] + \"test\" + train_dir[temp+5:]\n",
    "    \n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "        \n",
    "    classes = os.listdir(train_dir)\n",
    "        \n",
    "    train_dir = [train_dir + x + \"/\" for x in classes]\n",
    "    test_dir = [test_dir + x + \"/\" for x in classes]\n",
    "    \n",
    "    for folder in test_dir:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "    \n",
    "    for train_class, test_class in zip(train_dir, test_dir):\n",
    "        files = os.listdir(train_class)[:nb_images]\n",
    "        for file in files:\n",
    "            copyfile(train_class + file,test_class + file)\n",
    "            os.remove(train_class + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_train_test(30,\"cropped/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.01,\n",
    "    height_shift_range=0.01,\n",
    "    shear_range=0.0005,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79794 images belonging to 8 classes.\n",
      "Found 240 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "target_size=(224,224)\n",
    "train_generator = train_datagen.flow_from_directory('cropped_train_gen', batch_size=32, \n",
    "                                                    target_size=target_size, preprocess_input=preprocess_input)\n",
    "validation_generator = test_datagen.flow_from_directory('cropped_test', batch_size=32, target_size=target_size\n",
    "                                                        , preprocess_input=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = 64\n",
    "base = VGG16(include_top=False)\n",
    "img_input = Input(shape=(3,None,None))\n",
    "x = base(img_input)\n",
    "x =Convolution2D(out, 3,3, activation=\"relu\", border_mode=\"same\")(x)\n",
    "x =Convolution2D(out, 3,3, activation=\"relu\", border_mode=\"same\")(x)\n",
    "x =Convolution2D(out, 3,3, activation=\"relu\", border_mode=\"same\")(x)\n",
    "x =Convolution2D(8, 3,3, border_mode=\"same\")(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation(\"softmax\")(x)\n",
    "model = Model(img_input, x)\n",
    "for layer in base.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "base.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 3, None, None) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "vgg16 (Model)                    (None, 512, None, Non 14714688    input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, None, None 294976      vgg16[1][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, None, None 36928       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, None, None 36928       convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 8, None, None) 4616        convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "globalaveragepooling2d_1 (Global (None, 8)             0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 8)             0           globalaveragepooling2d_1[0][0]   \n",
      "====================================================================================================\n",
      "Total params: 15,088,136\n",
      "Trainable params: 373,448\n",
      "Non-trainable params: 14,714,688\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.01, decay=0.01)\n",
    "#opt = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-46:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 409, in data_generator_task\n",
      "    generator_output = next(generator)\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 474, in __next__\n",
      "    return self.next(*args, **kwargs)\n",
      "  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 637, in next\n",
      "    a = x.shape()\n",
      "TypeError: 'tuple' object is not callable\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6d8752662740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         nb_val_samples=240, callbacks=[remote])\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                      \u001b[1;34m'(x, y, sample_weight) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m                                      \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m                                      str(generator_output))\n\u001b[0m\u001b[1;32m   1482\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m                     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None"
     ]
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator,\n",
    "        samples_per_epoch=1024,\n",
    "        nb_epoch=100,\n",
    "        validation_data=validation_generator,\n",
    "        verbose=0,\n",
    "        nb_val_samples=240, callbacks=[remote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_pictures(train_folder, class_folder, generator,total_images, \n",
    "                        batch_size = 32, target_size = (224,224), color_mode = \"rgb\"):\n",
    "    #count = len(os.listdir(train_folder + \"/\" + class_folder))\n",
    "    count = 0  \n",
    "    gen_folder = train_folder + \"_gen/\"\n",
    "        \n",
    "    if not os.path.exists(gen_folder):\n",
    "        os.makedirs(gen_folder)\n",
    "        \n",
    "    if not os.path.exists(gen_folder + class_folder):\n",
    "        os.makedirs(gen_folder + class_folder)\n",
    "        \n",
    "    \n",
    "    progbar = tq(total = total_images, leave=False)\n",
    "    progbar.update(count)\n",
    "    \n",
    "    \n",
    "    for X in generator.flow_from_directory(train_folder, target_size=target_size,\n",
    "                                             classes=[class_folder], \n",
    "                                             save_to_dir=train_folder + \"_gen/\" + class_folder, \n",
    "                                             batch_size=batch_size, color_mode=color_mode):\n",
    "        count +=batch_size\n",
    "        progbar.update(batch_size)\n",
    "        if count >= total_images:\n",
    "            break\n",
    "    progbar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equilibrate_dataset(folder, nb_images_per_class, generator):\n",
    "    c = 0\n",
    "    while c < nb_images_per_class:\n",
    "        for string in os.listdir(folder):\n",
    "            create_new_pictures(folder, string, generator, 30000, 1, (64,64))\n",
    "        c += 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 1 classes.\n",
      "Found 277 images belonging to 1 classes.\n",
      "Found 94 images belonging to 1 classes.\n",
      "Found 71 images belonging to 1 classes.\n",
      "Found 103961 images belonging to 1 classes.\n",
      "Found 359 images belonging to 1 classes.\n",
      "Found 166 images belonging to 1 classes.\n",
      "Found 793 images belonging to 1 classes.\n",
      "Found 2500 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8953aa9eb134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mequilibrate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cropped_train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_datagen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-45fd9f42bda4>\u001b[0m in \u001b[0;36mequilibrate_dataset\u001b[0;34m(folder, nb_images_per_class, generator)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnb_images_per_class\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mcreate_new_pictures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m30000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-93bc4c2b5c24>\u001b[0m in \u001b[0;36mcreate_new_pictures\u001b[0;34m(train_folder, class_folder, generator, total_images, batch_size, target_size, color_mode)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                              \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_folder\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                              \u001b[0msave_to_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_folder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_gen/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclass_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                              batch_size=batch_size, color_mode=color_mode):\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m             img = load_img(os.path.join(self.directory, fname),\n\u001b[1;32m    690\u001b[0m                            \u001b[0mgrayscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                            target_size=self.target_size)\n\u001b[0m\u001b[1;32m    692\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_ordering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_ordering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Ensure 3 channel even when loaded image is grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                         \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "equilibrate_dataset(\"cropped_train\", 50000, test_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
