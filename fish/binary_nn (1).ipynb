{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "remote = callbacks.RemoteMonitor(root='http://localhost:9000')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.optimizers import *\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "import csv\n",
    "from sklearn.metrics import log_loss\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from convnetskeras.customlayers import Softmax4D\n",
    "import gc\n",
    "from convnetskeras.convnets import preprocess_image_batch, convnet\n",
    "from convnetskeras.imagenet_tool import synset_to_dfs_ids\n",
    "import PIL\n",
    "from PIL import Image,ImageFilter, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "from matplotlib.patches import Ellipse\n",
    "import tqdm as tq\n",
    "from numpy.linalg import eig, inv\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from skimage import data, io, filters\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "import math\n",
    "from skimage.transform import (hough_line, hough_line_peaks,\n",
    "                               probabilistic_hough_line)\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98943 images belonging to 2 classes.\n",
      "Found 98943 images belonging to 2 classes.\n",
      "Found 11800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "binary_folder_train = './binary_dataset/train/'\n",
    "binary_folder_train_fish = binary_folder_train+'fish'\n",
    "binary_folder_train_not_fish = binary_folder_train+'not_fish'\n",
    "binary_folder_test = './binary_dataset/test/'\n",
    "\n",
    "data_gen_args = dict(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.1,\n",
    "    )\n",
    "\n",
    "train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "validation_datagen = ImageDataGenerator(**data_gen_args)\n",
    "test_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(binary_folder_train, batch_size=16)\n",
    "validation_generator = validation_datagen.flow_from_directory(binary_folder_train, batch_size=16)\n",
    "test_generator = validation_datagen.flow_from_directory(binary_folder_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge, Input\n",
    "def return_model():\n",
    "    model = Sequential()\n",
    "    model.add(Activation(activation=\"relu\", input_shape=(3,None,None)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(32, 5, 5, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_resnet50():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024,init='normal', activation='relu')(x)\n",
    "    x = Dropout(0.005)(x)\n",
    "    x = Dense(256,init='normal', activation='relu')(x)\n",
    "    x = Dropout(0.005)(x)\n",
    "    x = Dense(256,init='normal', activation='relu')(x)\n",
    "    x = Dropout(0.005)(x)\n",
    "    x = Dense(128,init='normal', activation='relu')(x)\n",
    "    x = Dropout(0.005)(x)\n",
    "    predictions = Dense(2,init='normal', activation='softmax')(x)\n",
    "\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = return_resnet50()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "\n",
    "opt = SGD(lr=1e-1, decay=1e-6, momentum=0.2, nesterov=True)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "nb_epoch = 100\n",
    "model.fit_generator(train_generator,\n",
    "        samples_per_epoch=1024,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        verbose=0,\n",
    "        nb_val_samples=256, callbacks=[remote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(test_generator,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_2c_100e_R50_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_2c_100e_R50_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_2c_100e_R50_2.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers.core import  Lambda, Merge\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from os import listdir\n",
    "from os.path import isfile, join, dirname\n",
    "from scipy.io import loadmat\n",
    "import gc\n",
    "from keras.utils.layer_utils import layer_from_config\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "# Credits to heuritech for their great code which was a great inspiration.\n",
    "# Some of the code comes directly from their repository.\n",
    "# You can look it up: https://github.com/heuritech/convnets-keras\n",
    "\n",
    "\n",
    "def depthfirstsearch(id_, out=None):\n",
    "    if out is None:\n",
    "        out = []\n",
    "    if isinstance(id_, int):\n",
    "        pass\n",
    "    else:\n",
    "        id_ = next(int(s[0]) for s in synsets if s[1][0] == id_)\n",
    "        \n",
    "    out.append(id_)\n",
    "    children = synsets[id_-1][5][0]\n",
    "    for c in children:\n",
    "        depthfirstsearch(int(c), out)\n",
    "    return out\n",
    "\t\n",
    "# This is to find all the outputs that correspond to the class we want.\n",
    "def synset_to_dfs_ids(synset):\n",
    "    ids = [x for x in depthfirstsearch(synset) if x <= 1000]\n",
    "    ids = [corr[x] for x in ids]\n",
    "    return ids\n",
    "\t\n",
    "# Keras doesn't have a 4D softmax. So we need this.\n",
    "class Softmax4D(Layer):\n",
    "    def __init__(self, axis=-1,**kwargs):\n",
    "        self.axis=axis\n",
    "        super(Softmax4D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        e = K.exp(x - K.max(x, axis=self.axis, keepdims=True))\n",
    "        s = K.sum(e, axis=self.axis, keepdims=True)\n",
    "        return e / s\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\t\t\n",
    "\n",
    "def get_dim(model, layer_index, input_shape=None):\n",
    "    \n",
    "    # Input shape is the shape of images used during training.\n",
    "    if input_shape is not None:\n",
    "        dummy_vector = np.zeros((1,) + input_shape)\n",
    "    else:\n",
    "        if model.layers[0].input_shape[2] is None:\n",
    "            raise ValueError('You must provide \\\"input_shape = (3,256,256)\\\" for example when calling the function.')\n",
    "        dummy_vector = np.zeros((1,) + model.layers[0].input_shape[1:])\n",
    "    \n",
    "    intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.layers[layer_index].output)\n",
    "    \n",
    "    out = intermediate_layer_model.predict(dummy_vector)\n",
    "    \n",
    "    return out.shape[1:]\n",
    "\t\n",
    "\n",
    "def from_config(layer, config_dic):\n",
    "    config_correct = {}\n",
    "    config_correct['class_name'] = type(layer)\n",
    "    config_correct['config'] = config_dic\n",
    "    return layer_from_config(config_correct)\n",
    "\t\n",
    "\n",
    "def add_to_model(x, layer):\n",
    "    new_layer = from_config(layer, layer.get_config())\n",
    "    x = new_layer(x)\n",
    "    if layer.get_weights() is not None:\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "    return x\n",
    "\t\n",
    "\n",
    "def layer_type(layer):\n",
    "    return str(layer)[10:].split(\" \")[0].split(\".\")[-1]\n",
    "\t\n",
    "\n",
    "def detect_configuration(model):\n",
    "    # must return the configuration and the number of the first pooling layer\n",
    "    \n",
    "    # Names (types) of layers from end to beggining\n",
    "    inverted_list_layers = [layer_type(layer) for layer in model.layers[::-1]]\n",
    "    \n",
    "    layer1 = None\n",
    "    layer2 = None \n",
    "    \n",
    "    i = len(model.layers)\n",
    "    \n",
    "    for layer in inverted_list_layers:\n",
    "        i -= 1\n",
    "        if layer2 is None:\n",
    "            if layer == \"GlobalAveragePooling2D\" or layer == \"GlobalMaxPooling2D\":\n",
    "                layer2 = layer\n",
    "\n",
    "            elif layer == \"Flatten\":\n",
    "                return \"local pooling - flatten\", i-1\n",
    "            \n",
    "        else:\n",
    "            layer1 = layer\n",
    "            break\n",
    "            \n",
    "    if layer1 == \"MaxPooling2D\" and layer2 == \"GlobalMaxPooling2D\":\n",
    "        return \"local pooling - global pooling (same type)\", i\n",
    "    elif layer1 == \"AveragePooling2D\" and layer2 == \"GlobalAveragePooling2D\":\n",
    "        return \"local pooling - global pooling (same type)\", i\n",
    "    \n",
    "    elif layer1 == \"MaxPooling2D\" and layer2 == \"GlobalAveragePooling2D\":\n",
    "        return \"local pooling - global pooling (different type)\", i+1\n",
    "    elif layer1 == \"AveragePooling2D\" and layer2 == \"GlobalMaxPooling2D\":\n",
    "        return \"local pooling - global pooling (different type)\", i+1\n",
    "    \n",
    "    else:\n",
    "        return \"global pooling\", i\n",
    "\t\t\n",
    "    \n",
    "def add_zeros(w, nb_zeros):\n",
    "    \n",
    "    n = w.shape[3]\n",
    "    indexes = np.array(range(1, n))\n",
    "    w1 = w\n",
    "    for i in range(nb_zeros):\n",
    "        w1 = np.insert(w1, indexes + i, 0, axis=2)\n",
    "    for i in range(nb_zeros):\n",
    "        w1 = np.insert(w1, indexes + i, 0, axis=3)\n",
    "    return w1\n",
    "\t\n",
    "    \n",
    "def insert_weights(layer, new_layer):\n",
    "    W,b = layer.get_weights()\n",
    "    n_filter,previous_filter,ax1,ax2 = new_layer.get_weights()[0].shape\n",
    "    ax1 = ax2 = int(np.sqrt(layer.get_weights()[0].shape[0]/new_layer.get_weights()[0].shape[1]))\n",
    "    new_W = W.reshape((previous_filter,ax1,ax2,n_filter))\n",
    "    new_W = new_W.transpose((3,0,1,2))\n",
    "    new_W = new_W[:,:,::-1,::-1]\n",
    "\t\n",
    "    \n",
    "    if ax1!=1:\n",
    "        insert_zeros = int((new_layer.get_weights()[0].shape[2] - ax1)/(ax1-1))\n",
    "        print(\"insert_zeros=\" + str(insert_zeros))\n",
    "        new_W =  add_zeros(new_W, insert_zeros)\n",
    "    \n",
    "    new_layer.set_weights([new_W,b])\n",
    "\t\n",
    "    \n",
    "def copy_last_layers(model, begin,x):\n",
    "    \n",
    "    i=begin\n",
    "    \n",
    "    for layer in model.layers[begin:]:\n",
    "        if layer_type(layer) == \"Dense\":\n",
    "            \n",
    "            if i == len(model.layers)-1:\n",
    "                x = add_reshaped_layer(layer,x,1, no_activation=True)\n",
    "            else:\n",
    "                x = add_reshaped_layer(layer,x,1)\n",
    "            \n",
    "        elif layer_type(layer) == \"Dropout\":\n",
    "            pass\n",
    "                \n",
    "        elif layer_type(layer) == \"Activation\" and i == len(model.layers)-1:\n",
    "            break\n",
    "               \n",
    "        else:\n",
    "            x = add_to_model(x, layer)\n",
    "        i+=1\n",
    "    \n",
    "    x = Softmax4D(axis=1,name=\"softmax\")(x)\n",
    "    return x\n",
    "    \n",
    "                \n",
    "def add_reshaped_layer(layer, x, size, no_activation=False, add_zeros = None):\n",
    "\n",
    "    conf = layer.get_config()\n",
    "    \n",
    "    if no_activation:\n",
    "        activation=\"linear\"\n",
    "    else:\n",
    "        activation=conf[\"activation\"]\n",
    "        \n",
    "    #size = int(np.sqrt(layer.get_weights()[0].shape[0]/conf[\"output_dim\"]))\n",
    "    \n",
    "    new_layer = Convolution2D(conf[\"output_dim\"],size,size, activation=activation, name=conf['name'])\n",
    "         \n",
    "        \n",
    "    x= new_layer(x)\n",
    "    # We transfer the weights:\n",
    "    insert_weights(layer, new_layer)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def to_heatmap(model, input_shape = None, delete = False):\n",
    "    \n",
    "    # there are four configurations possible:\n",
    "    # global pooling\n",
    "    # local pooling - flatten\n",
    "    # local pooling - global pooling (same type)\n",
    "    # local pooling - global pooling (different type)\n",
    "    \n",
    "    model_type, index = detect_configuration(model)\n",
    "    \n",
    "    print(\"Model type detected: \" + model_type)\n",
    "    \n",
    "    #new_layer.set_weights(model.layers[0].get_weights())\n",
    "    img_input = Input(shape=(3,None,None))\n",
    "   \n",
    "    # Inchanged part:\n",
    "    middle_model = Model(input=model.layers[1].input, output=model.layers[index-1].output)\n",
    "    \n",
    "    x = middle_model(img_input)\n",
    "    \n",
    "    print(\"Model cut at layer: \" + str(index))\n",
    "        \n",
    "    if model_type == \"global pooling\":\n",
    "        x = copy_last_layers(model, index+1,x)\n",
    "              \n",
    "    elif model_type == \"local pooling - flatten\":\n",
    "        \n",
    "        layer = model.layers[index]\n",
    "        dic = layer.get_config()\n",
    "        add_zeros = dic[\"strides\"][0] - 1\n",
    "        dic[\"strides\"] = (1,1)\n",
    "        new_pool = from_config(layer, dic)\n",
    "        x = new_pool(x)\n",
    "        \n",
    "        size = get_dim(model, index, input_shape)[1]\n",
    "        print(\"Pool size infered: \" + str(size))\n",
    "        \n",
    "        conv_size = size + (size-1) * add_zeros\n",
    "        \n",
    "        print(\"New convolution size: \" + str(conv_size))\n",
    "        \n",
    "        if index+2 != len(model.layers)-1:\n",
    "            x = add_reshaped_layer(model.layers[index+2],x,conv_size, add_zeros=add_zeros)\n",
    "        else:\n",
    "            x = add_reshaped_layer(model.layers[index+2],x,conv_size, add_zeros=add_zeros,no_activation=True)\n",
    "            \n",
    "        x = copy_last_layers(model, index+3,x)\n",
    "        \n",
    "        \n",
    "    elif model_type == \"local pooling - global pooling (same type)\":\n",
    "        \n",
    "        \n",
    "        dim = get_dim(model, index, input_shape=input_shape)\n",
    "\n",
    "        new_pool_size = model.layers[index].get_config()[\"pool_size\"][0] * dim[1]\n",
    "        \n",
    "        print(\"Pool size infered: \" + str(new_pool_size))\n",
    "        \n",
    "        x = AveragePooling2D(pool_size=(new_pool_size, new_pool_size), strides=(1,1)) (x)\n",
    "        x = copy_last_layers(model, index+2,x)\n",
    "        \n",
    "        \n",
    "    elif model_type == \"local pooling - global pooling (different type)\":\n",
    "        x= copy_last_layers(model, index+1,x)\n",
    "    else:\n",
    "        raise IndexError(\"no type for model: \" + str(model_type))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if delete:\n",
    "        del(model)\n",
    "        gc.collect()\n",
    "        print(\"Original model was deleted.\")\n",
    "    \n",
    "    return Model(img_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_2c_10e_R50_1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model_2c_10e_R50_1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_model = to_heatmap(model, input_shape = (3,256,256), delete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "res_model = ResNet50(weights='imagenet', include_top=True,input_shape = (3,224,224))\n",
    "new_res_model = to_heatmap(res_model, input_shape = (3,224,224), delete=False)\n",
    "new_res_model.compile(optimizer=sgd, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_path = \"./train/train/ALB/\"\n",
    "file = os.listdir(root_path)\n",
    "N = int(np.random.uniform(0,1)*len(file))\n",
    "print(N)\n",
    "plt.figure(0)\n",
    "img_path = root_path+file[N]\n",
    "img = image.load_img(img_path)\n",
    "plt.imshow(img)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "out = new_model.predict(x)\n",
    "plt.figure(1)\n",
    "heatmap = out[0,[1]].sum(axis=0)\n",
    "#p2, p98 = np.percentile(heatmap, (2, 98))\n",
    "#heatmap = exposure.rescale_intensity(heatmap_out, in_range=(p2, p98))\n",
    "plt.imshow(heatmap, interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_mask_path(img_path,th,model_name):\n",
    "    def binarize(value, threshold):\n",
    "        if value < threshold:\n",
    "            return 255\n",
    "        else:\n",
    "            return 0\n",
    "    json_file = open(model_name+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights(model_name+\".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    img = image.load_img(img_path)\n",
    "    plt.imshow(img)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    out = new_model.predict(x)\n",
    "    heatmap = out[0,[1]].sum(axis=0)\n",
    "    b = np_binarize(heatmap, th)\n",
    "    import scipy\n",
    "    size = np.array(img.size)\n",
    "    b = scipy.misc.imresize(b, [size[1],size[0]], interp='bilinear', mode=None)\n",
    "    img_array = np.array(img)\n",
    "    bb = b >= 2\n",
    "    mask = np.zeros_like(img_array)\n",
    "    for i in range(3):\n",
    "        mask[:,:,i] = bb\n",
    "    idx=(mask==0)\n",
    "    img_array[idx]=mask[idx]\n",
    "    img_filter = Image.fromarray(img_array)\n",
    "    return img_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binarize(value, threshold,state=True):\n",
    "    if state:\n",
    "        if value < threshold:\n",
    "            return 255\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if value < threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return 255        \n",
    "def filter_mask(img,heatmap,th,state):\n",
    "    np_binarize = np.vectorize(binarize)\n",
    "    bb = np_binarize(heatmap, th,state)\n",
    "    import scipy\n",
    "    size = np.array(img.size)\n",
    "    b = scipy.misc.imresize(bb, [size[1],size[0]], interp='bicubic', mode=None)\n",
    "    img_array = np.array(img)\n",
    "    bb = b >= 2\n",
    "    mask = np.zeros_like(img_array)\n",
    "    for i in range(3):\n",
    "        mask[:,:,i] = bb\n",
    "    idx=(mask==0)\n",
    "    img_array[idx]=mask[idx]\n",
    "    img_filter = Image.fromarray(img_array)\n",
    "    return img_filter,Image.fromarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "#img_f = filter_mask(img_path,0.999,'model_2c_100e_R50_2')\n",
    "img_f,b1 = filter_mask(img,heatmap,0.999,True)\n",
    "#img_f = filter_mask(img_f,heatmap2,0.999)\n",
    "plt.figure()\n",
    "plt.imshow(img_f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from convnetskeras.imagenet_tool import synset_to_dfs_ids\n",
    "x = image.img_to_array(img_f)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "out = new_res_model.predict(x)\n",
    "\n",
    "s = \"n02512053\"\n",
    "ids = synset_to_dfs_ids(s)\n",
    "heatmap_out = out[0,ids].sum(axis=0)\n",
    "#p2, p98 = np.percentile(heatmap_out, (2, 98))\n",
    "#heatmap_out = exposure.rescale_intensity(heatmap_out, in_range=(p2, p98))\n",
    "plt.figure()\n",
    "plt.imshow(heatmap_out, interpolation='none')\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "out = new_res_model.predict(x)\n",
    "\n",
    "s = \"n02512053\"\n",
    "ids = synset_to_dfs_ids(s)\n",
    "heatmap_out2 = out[0,ids].sum(axis=0)\n",
    "#p2, p98 = np.percentile(heatmap_out, (2, 98))\n",
    "#heatmap_out = exposure.rescale_intensity(heatmap_out, in_range=(p2, p98))\n",
    "plt.figure()\n",
    "plt.imshow(heatmap_out2, interpolation='none')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(img_f)\n",
    "#img_f = filter_mask(img_path,0.999,'model_2c_100e_R50_2')\n",
    "img_f2,b2 = filter_mask(img_f,heatmap_out,0.0001,False)\n",
    "#img_f = filter_mask(img_f,heatmap2,0.1)\n",
    "plt.figure()\n",
    "plt.imshow(b2)\n",
    "plt.figure()\n",
    "plt.imshow(img_f2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_class(value, old, new):\n",
    "    if value == old:\n",
    "        return new\n",
    "    else:\n",
    "        return value\n",
    "np_change_class = np.vectorize(change_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stuff(mat,i,j,islands_list, count, islands_matrix):\n",
    "    if mat[i][j] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        before = int(islands_matrix[i][j-1])\n",
    "        above = int(islands_matrix[i-1][j])\n",
    "        if before == 0:\n",
    "            if  above== 0:\n",
    "                islands_matrix[i][j] = count\n",
    "                count +=1\n",
    "                islands_list.append([(j,i)])\n",
    "            else: \n",
    "                islands_matrix[i][j] = above\n",
    "                islands_list[above-1].append((j,i))\n",
    "        \n",
    "        else:\n",
    "            if above == 0:\n",
    "                islands_matrix[i][j] = before\n",
    "                islands_list[before-1].append((j,i))\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                if above == before:\n",
    "                    islands_matrix[i][j] = before\n",
    "                    islands_list[before-1].append((j,i))\n",
    "                \n",
    "                #it's on.\n",
    "                else:\n",
    "                    old = max((above,before))\n",
    "                    new = min((above,before))\n",
    "                    \n",
    "                    islands_matrix[i][j] = new\n",
    "                    islands_matrix = np_change_class(islands_matrix, old, new)\n",
    "                    islands_list[new-1] += islands_list[old-1]\n",
    "                    islands_list[new-1].append((j,i))\n",
    "                    islands_list[old-1] = []\n",
    "                    \n",
    "    return islands_list, count, islands_matrix        \n",
    "\n",
    "\n",
    "# The not fun part\n",
    "def group_by_island(matrix):\n",
    "    \n",
    "    h, l = np.shape(matrix)\n",
    "    border = np.array([[0]*(l)])\n",
    "    mat = np.concatenate((border,matrix), axis=0)\n",
    "    border = np.array([[0]*(h+1)])\n",
    "    mat = np.concatenate((border.T, mat), axis=1)\n",
    "    \n",
    "    islands_matrix =np.zeros((h+1,l+1))\n",
    "    count = 1\n",
    "    islands_list = []\n",
    "    \n",
    "    for i in range(1,h+1):\n",
    "        for j in range(1, l+1):\n",
    "            islands_list, count, islands_matrix = stuff(mat,i,j,islands_list, count, islands_matrix)\n",
    "\n",
    "    \n",
    "    \n",
    "    return islands_matrix, islands_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "islands_matrix, islands_list = group_by_island(b2)\n",
    "plt.imshow(islands_matrix, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_extremes(list_coordinates):\n",
    "    max_x = max(list_coordinates, key = lambda x: x[0])[0]\n",
    "    min_x = min(list_coordinates, key = lambda x: x[0])[0]\n",
    "    max_y = max(list_coordinates, key = lambda x: x[1])[1]\n",
    "    min_y = min(list_coordinates, key = lambda x: x[1])[1]\n",
    "    \n",
    "    #print(min_x, min_y, max_x, max_y )\n",
    "    \n",
    "    return min_x - 1, min_y - 1, max_x-min_x + 1, max_y-min_y + 1\n",
    "\n",
    "\n",
    "def get_rectangles(islands_list, area_limit=4):\n",
    "    list_coordinates = []\n",
    "    for island in islands_list:\n",
    "        if len(island) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            coordinates = find_extremes(island)\n",
    "            #print(coordinates)\n",
    "            #print(coordinates[2]*coordinates[3])\n",
    "            if coordinates[2]*coordinates[3] >=area_limit:\n",
    "                #print(coordinates)\n",
    "                list_coordinates.append(coordinates)\n",
    "            \n",
    "    return list_coordinates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rectangles = get_rectangles(islands_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fish_th = 0.99\n",
    "for rect in rectangles:\n",
    "    x = rect[0]\n",
    "    y = rect[1]\n",
    "    w = rect[2]\n",
    "    h = rect[3]\n",
    "    img_crop = img.crop((x,y,x+w,y+h))\n",
    "    plt.figure()\n",
    "    plt.imshow(img_crop)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_min_dim(size):\n",
    "    w,h = size\n",
    "    while w > 150 or h > 150:\n",
    "        w/=2\n",
    "        h/=2\n",
    "    print(w,h)\n",
    "    return int(w),int(h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import data, color\n",
    "from skimage.feature import canny\n",
    "from skimage.transform import hough_ellipse\n",
    "from skimage.draw import ellipse_perimeter\n",
    "size = img_crop.size\n",
    "img_to_use = img_crop.resize(get_min_dim(size), PIL.Image.ANTIALIAS)\n",
    "plt.imshow(img_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load picture, convert to grayscale and detect edges\n",
    "image_rgb = np.array(img_to_use)\n",
    "image_gray = color.rgb2gray(image_rgb)\n",
    "edges = filters.scharr(image_gray)\n",
    "p2, p98 = np.percentile(edges, (1, 99))\n",
    "img_rescale = exposure.rescale_intensity(edges, in_range=(p2, p98))\n",
    "plt.imshow(edges)\n",
    "th = 0.6\n",
    "image_0 = []\n",
    "Max = 0\n",
    "for line in img_rescale:\n",
    "    obj = []\n",
    "    for y in line:\n",
    "        if y > Max:\n",
    "            Max = y\n",
    "        if y > th:\n",
    "            obj.append(255)\n",
    "        else:\n",
    "            obj.append(0)    \n",
    "    image_0.append(obj)\n",
    "image_0 = Image.fromarray(np.array(image_0))\n",
    "#image_1 = ndimage.binary_erosion(image_0,structure=np.ones((2,2)))\n",
    "plt.figure(4)\n",
    "plt.imshow(image_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Perform a Hough Transform\n",
    "# The accuracy corresponds to the bin size of a major axis.\n",
    "# The value is chosen in order to get a single high accumulator.\n",
    "# The threshold eliminates low accumulators\n",
    "result = hough_ellipse(np.array(image_0), accuracy=20, threshold=10,\n",
    "                       min_size=10, max_size=50)\n",
    "result.sort(order='accumulator')\n",
    "\n",
    "# Estimated parameters for the ellipse\n",
    "print(result)\n",
    "best = list(result[-1])\n",
    "yc, xc, a, b = [int(round(x)) for x in best[1:5]]\n",
    "orientation = best[5]\n",
    "\n",
    "# Draw the ellipse on the original image\n",
    "cy, cx = ellipse_perimeter(yc, xc, a, b, orientation)\n",
    "image_rgb[cy, cx] = (0, 0, 255)\n",
    "# Draw the edge (white) and the resulting ellipse (red)\n",
    "edges = color.gray2rgb(edges)\n",
    "edges[cy, cx] = (250, 0, 0)\n",
    "\n",
    "fig2, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(8, 4), sharex=True,\n",
    "                                sharey=True,\n",
    "                                subplot_kw={'adjustable':'box-forced'})\n",
    "\n",
    "ax1.set_title('Original picture')\n",
    "ax1.imshow(image_rgb)\n",
    "\n",
    "ax2.set_title('Edge (white) and result (red)')\n",
    "ax2.imshow(edges)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def get_mask_from_img(classe,img_name,size):\n",
    "    import os\n",
    "    import collections\n",
    "    import json\n",
    "    import numpy as np\n",
    "    json_name = classe.lower()+'_labels.json'\n",
    "    data_json = collections.defaultdict(int)\n",
    "    with open(json_name,'r+') as f:\n",
    "        labels = json.load(f)\n",
    "        for label in labels:\n",
    "            name = label['filename'].split('/')[-1]\n",
    "            if img_name == name:\n",
    "                rects = label['annotations']\n",
    "                mask = np.zeros((100,100))\n",
    "                if len(rects) > 0 :\n",
    "                    x_r = 100/size[0]\n",
    "                    y_r = 100/size[1]\n",
    "                    for rect in rects:\n",
    "                        x = rect['x']*x_r\n",
    "                        y = rect['y']*y_r\n",
    "                        w = rect['width']*x_r\n",
    "                        h = rect['height']*y_r\n",
    "                        for n in range(size[0]-1):\n",
    "                            for k in range(size[1]-1):\n",
    "                                if n > x and n < (x+w) and k > y and k < (y+h) and n < 100 and k < 100:\n",
    "                                        mask[k][n] = 1\n",
    "                   # img_array[idx]=mask[idx]\n",
    "                    return mask\n",
    "                else:\n",
    "                    return mask    \n",
    "plt.imshow(get_mask_from_img('ALB','img_00010.jpg',(1280,720)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
