{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TRAIN_PATH = \"./train\"\n",
    "RELABELS_PATH = \"relabels.csv\"\n",
    "\n",
    "os.mkdir(\"{}/{}\".format(TRAIN_PATH, \"revise\"))\n",
    "\n",
    "with open(RELABELS_PATH) as f:\n",
    "    for line in f:\n",
    "        cols = line.split()\n",
    "        src = \"{}/{}/{}.jpg\".format(TRAIN_PATH, cols[1], cols[0])\n",
    "        dst = \"{}/{}/{}.jpg\".format(TRAIN_PATH, cols[2], cols[0])\n",
    "\n",
    "        try:\n",
    "            os.rename(src, dst)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"{} not found\".format(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from shutil import copyfile, copytree\n",
    "import time\n",
    "import gc \n",
    "from scipy.misc import imread\n",
    "import cv2\n",
    "from sklearn import cluster\n",
    "import re\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = \"./train/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need to make a backup of the train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train_clustered/'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copytree(TRAIN_FOLDER, TRAIN_FOLDER[:-1] + \"_clustered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = TRAIN_FOLDER[:-1] + \"_clustered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = glob.glob(TRAIN_FOLDER + \"*/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first do a classification with the images sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes ~3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a directory if it doesn't exist\n",
    "def mk(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6b13bc5b0811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtemp_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdestination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{}{}_{}/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    \n",
    "    folder = path[:-13]\n",
    "    temp_img = imread(path)\n",
    "    destination = \"{}{}_{}/\".format(folder, temp_img.shape[0], temp_img.shape[1])\n",
    "    mk(destination)\n",
    "    \n",
    "    os.rename(path, destination + path[-13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll group images using dbscan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of code is taken from this notebook: https://www.kaggle.com/anokas/the-nature-conservancy-fisheries-monitoring/finding-boatids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells take ~20min to compute (computing the distance matrix in n-square complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(img, img2):\n",
    "    return np.mean(np.abs(img - img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_matrices(mat1, mat2):\n",
    "    result = np.mean(np.abs(mat2 - mat1),axis=2)  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We loop over the folder containing the images.\n",
    "folders_img_sizes = glob.glob(TRAIN_FOLDER + \"*/*/\")\n",
    "\n",
    "\n",
    "print(\"Nb of folders to do:\", len(folders_img_sizes))\n",
    "\n",
    "for folder in glob.glob(TRAIN_FOLDER + \"*/*/\"):\n",
    "    train_files = glob.glob(folder + \"*.jpg\")\n",
    "    train = np.array([imread(img) for img in train_files])\n",
    "\n",
    "\n",
    "    # Resize the images to speed it up.\n",
    "    train = [cv2.resize(img, (224, 224), cv2.INTER_LINEAR) for img in train]\n",
    "\n",
    "    custom_train = []\n",
    "    for img in train:\n",
    "        im = (img - img.mean()) / img.std()\n",
    "        custom_train.append(im.reshape((224*224*3,)))\n",
    "\n",
    "    train = np.array(custom_train)\n",
    "\n",
    "    l = len(train)\n",
    "    \n",
    "    print(l, \"images in\", folder,  \"to create clusters.\")\n",
    "    \n",
    "    if l <=4:\n",
    "        continue\n",
    "    \n",
    "    distances = np.zeros((l,l))\n",
    "    \n",
    "    # We compute the distance matrix\n",
    "    for i in range(l):\n",
    "        for j in range(l):\n",
    "            distances[i,j] = compare(train[i], train[j])\n",
    "    \n",
    "    # We compute the clusters\n",
    "    cls = cluster.DBSCAN(metric='precomputed', min_samples=3, eps=0.1)\n",
    "    y = cls.fit_predict(distances)\n",
    "    \n",
    "    # We move the images in the good folder.\n",
    "    for path, cluster_idx in zip(train_files, y.tolist()):\n",
    "        dest_folder = folder + str(cluster_idx) + \"/\"\n",
    "        mk(dest_folder)\n",
    "        dest_path = dest_folder + path[-13:]\n",
    "        os.rename(path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the JSON file for easy sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COPIES_FOLDER = \"./copies/\"\n",
    "\n",
    "images_kept = os.listdir(COPIES_FOLDER)\n",
    "\n",
    "images_removed = {}\n",
    "\n",
    "for img_kept in images_kept:\n",
    "    path_folder = COPIES_FOLDER + img_kept + \"/\"\n",
    "    sequence_removed = os.listdir(path_folder)\n",
    "\n",
    "    images_removed[img_kept] = sequence_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('duplicates.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(images_removed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the JSON to clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FOLDER = \"./train/\"\n",
    "JSON_PATH = \"duplicates.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(JSON_PATH) as data_file:    \n",
    "    duplicate_dict = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to be able to find the path of a file:\n",
    "\n",
    "path_dictionary = {}\n",
    "\n",
    "for class_folder in os.listdir(TRAIN_FOLDER):\n",
    "    files = os.listdir(TRAIN_FOLDER + class_folder)\n",
    "    \n",
    "    for file in files:\n",
    "        path_dictionary[file] = TRAIN_FOLDER + class_folder + \"/\" + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duplicate_folder = TRAIN_FOLDER + \"duplicates/\"\n",
    "mk(duplicate_folder)\n",
    "\n",
    "for file, duplicates in duplicate_dict.items():\n",
    "    try:\n",
    "        path = path_dictionary[file]\n",
    "    except KeyError:\n",
    "        print(\"Couldn't find\", file, \"in the train folder.\")\n",
    "        continue\n",
    "    mk(duplicate_folder + path.split(\"/\")[-2])    \n",
    "\n",
    "    dest_folder = duplicate_folder + path.split(\"/\")[-2] + \"/\" + file\n",
    "    mk(dest_folder)\n",
    "\n",
    "    for duplicate in duplicates:\n",
    "        \n",
    "        try:\n",
    "            path = path_dictionary[duplicate]\n",
    "        except KeyError:\n",
    "            print(\"Couldn't find\", file, \"in the train folder.\")\n",
    "            continue\n",
    "    \n",
    "        new_path = dest_folder + \"/\" + duplicate\n",
    "\n",
    "        os.rename(path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = \"./test1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = glob.glob(f + \"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    \n",
    "    folder = path[:-13]\n",
    "    temp_img = imread(path)\n",
    "    destination = \"{}{}_{}/\".format(folder, temp_img.shape[0], temp_img.shape[1])\n",
    "    mk(destination)\n",
    "    \n",
    "    os.rename(path, destination + path[-13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of folders to do: 10\n",
      "45 images in ./test1\\670_1192\\ to create clusters.\n",
      "4 images in ./test1\\700_1244\\ to create clusters.\n",
      "57 images in ./test1\\718_1276\\ to create clusters.\n",
      "606 images in ./test1\\720_1280\\ to create clusters.\n",
      "148 images in ./test1\\750_1280\\ to create clusters.\n",
      "11 images in ./test1\\750_1334\\ to create clusters.\n",
      "21 images in ./test1\\854_1518\\ to create clusters.\n",
      "7 images in ./test1\\924_1280\\ to create clusters.\n",
      "98 images in ./test1\\974_1280\\ to create clusters.\n",
      "3 images in ./test1\\974_1732\\ to create clusters.\n"
     ]
    }
   ],
   "source": [
    "# We loop over the folder containing the images.\n",
    "folders_img_sizes = glob.glob(f + \"*/\")\n",
    "\n",
    "\n",
    "print(\"Nb of folders to do:\", len(folders_img_sizes))\n",
    "\n",
    "for folder in glob.glob(f + \"*/\"):\n",
    "    train_files = glob.glob(folder + \"*.jpg\")\n",
    "    train = np.array([imread(img) for img in train_files])\n",
    "\n",
    "\n",
    "    # Resize the images to speed it up.\n",
    "    train = [cv2.resize(img, (224, 224), cv2.INTER_LINEAR) for img in train]\n",
    "\n",
    "    custom_train = []\n",
    "    for img in train:\n",
    "        im = (img - img.mean()) / img.std()\n",
    "        custom_train.append(im.reshape((224*224*3,)))\n",
    "\n",
    "    train = np.array(custom_train)\n",
    "\n",
    "    l = len(train)\n",
    "    \n",
    "    print(l, \"images in\", folder,  \"to create clusters.\")\n",
    "    \n",
    "    if l <=4:\n",
    "        continue\n",
    "    \n",
    "    distances = np.zeros((l,l))\n",
    "    \n",
    "    # We compute the distance matrix\n",
    "    for i in range(l):\n",
    "        for j in range(l):\n",
    "            distances[i,j] = compare(train[i], train[j])\n",
    "    \n",
    "    # We compute the clusters\n",
    "    cls = cluster.DBSCAN(metric='precomputed', min_samples=3, eps=0.2)\n",
    "    y = cls.fit_predict(distances)\n",
    "    \n",
    "    # We move the images in the good folder.\n",
    "    for path, cluster_idx in zip(train_files, y.tolist()):\n",
    "        dest_folder = folder + str(cluster_idx) + \"/\"\n",
    "        mk(dest_folder)\n",
    "        dest_path = dest_folder + path[-13:]\n",
    "        os.rename(path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
